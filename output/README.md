# Output Directory

This directory contains all outputs generated by the UNSW-NB15 Hadoop analytics environment, including analysis results, visualizations, and system logs.

## üìÅ Directory Structure

```
output/
‚îú‚îÄ‚îÄ README.md                    # This file
‚îú‚îÄ‚îÄ logs/                        # System and application logs
‚îÇ   ‚îú‚îÄ‚îÄ setup.log               # Environment setup logs
‚îÇ   ‚îú‚îÄ‚îÄ data_loading.log         # Data loading process logs
‚îÇ   ‚îú‚îÄ‚îÄ query_execution.log      # HiveQL query execution logs
‚îÇ   ‚îî‚îÄ‚îÄ error.log               # Error messages and debugging info
‚îú‚îÄ‚îÄ results/                     # Query results and analysis outputs
‚îÇ   ‚îú‚îÄ‚îÄ query_results/          # HiveQL query output files
‚îÇ   ‚îú‚îÄ‚îÄ statistical_analysis/   # Statistical analysis results
‚îÇ   ‚îú‚îÄ‚îÄ ml_models/              # Machine learning model outputs
‚îÇ   ‚îî‚îÄ‚îÄ processed_data/         # Cleaned and processed datasets
‚îî‚îÄ‚îÄ visualizations/             # Generated charts and dashboards
    ‚îú‚îÄ‚îÄ static/                 # Static images (PNG, JPG, PDF)
    ‚îú‚îÄ‚îÄ interactive/            # Interactive HTML dashboards
    ‚îî‚îÄ‚îÄ reports/               # Generated analysis reports
```

## üìä Generated Content Types

### 1. Logs Directory (`logs/`)

#### System Logs
- **`setup.log`**: Complete environment setup process
- **`data_loading.log`**: Data loading and validation process
- **`service_health.log`**: Container health checks and monitoring

#### Application Logs
- **`hive_queries.log`**: HiveQL query execution details
- **`jupyter_sessions.log`**: Jupyter notebook execution logs
- **`python_analytics.log`**: Python script execution logs

#### Error Tracking
- **`error.log`**: Consolidated error messages
- **`debug.log`**: Detailed debugging information
- **`performance.log`**: Query performance metrics

### 2. Results Directory (`results/`)

#### Query Results (`query_results/`)
- **`attack_patterns_YYYYMMDD_HHMMSS.csv`**: Attack pattern analysis results
- **`temporal_analysis_YYYYMMDD_HHMMSS.csv`**: Time-based analysis outputs
- **`anomaly_detection_YYYYMMDD_HHMMSS.csv`**: Anomaly detection results
- **`vulnerability_assessment_YYYYMMDD_HHMMSS.csv`**: Service vulnerability data

#### Statistical Analysis (`statistical_analysis/`)
- **`correlation_matrix_YYYYMMDD_HHMMSS.csv`**: Feature correlation results
- **`distribution_analysis_YYYYMMDD_HHMMSS.json`**: Statistical distributions
- **`hypothesis_tests_YYYYMMDD_HHMMSS.json`**: Statistical test results

#### Machine Learning (`ml_models/`)
- **`anomaly_model_YYYYMMDD_HHMMSS.pkl`**: Trained anomaly detection models
- **`classification_model_YYYYMMDD_HHMMSS.pkl`**: Attack classification models
- **`feature_importance_YYYYMMDD_HHMMSS.csv`**: Feature importance rankings
- **`model_performance_YYYYMMDD_HHMMSS.json`**: Model evaluation metrics

### 3. Visualizations Directory (`visualizations/`)

#### Static Charts (`static/`)
- **`attack_distribution_YYYYMMDD_HHMMSS.png`**: Attack category charts
- **`protocol_analysis_YYYYMMDD_HHMMSS.png`**: Protocol usage analysis
- **`temporal_patterns_YYYYMMDD_HHMMSS.png`**: Time-based pattern charts
- **`anomaly_detection_YYYYMMDD_HHMMSS.png`**: Anomaly visualization charts

#### Interactive Dashboards (`interactive/`)
- **`security_dashboard_YYYYMMDD_HHMMSS.html`**: Complete security dashboard
- **`attack_analysis_YYYYMMDD_HHMMSS.html`**: Interactive attack analysis
- **`network_topology_YYYYMMDD_HHMMSS.html`**: Network visualization

#### Reports (`reports/`)
- **`summary_report_YYYYMMDD_HHMMSS.pdf`**: Automated analysis reports
- **`executive_summary_YYYYMMDD_HHMMSS.pdf`**: High-level findings
- **`technical_report_YYYYMMDD_HHMMSS.pdf`**: Detailed technical analysis

## üîÑ Automated Output Generation

### Jupyter Notebook Outputs
When running analytics notebooks, outputs are automatically saved:

```python
# Example: Saving analysis results
import pandas as pd
from datetime import datetime

# Generate timestamp for unique filenames
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Save query results
results_df.to_csv(f'output/results/query_results/custom_analysis_{timestamp}.csv', index=False)

# Save visualizations
plt.savefig(f'output/visualizations/static/custom_chart_{timestamp}.png', dpi=300, bbox_inches='tight')

# Save interactive plots
fig.write_html(f'output/visualizations/interactive/dashboard_{timestamp}.html')
```

### HiveQL Query Outputs
Query results can be exported using Hive:

```sql
-- Export query results to files
INSERT OVERWRITE DIRECTORY '/user/output/query_results'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
SELECT * FROM (
    -- Your complex analytical query here
    SELECT attack_cat, proto, COUNT(*) as attack_count
    FROM network_flows 
    WHERE label = true 
    GROUP BY attack_cat, proto
) results;
```

### Python Script Outputs
Automated scripts save results with timestamps:

```bash
# Run visualization generator
cd python/
python visualizations.py

# Outputs saved to:
# - output/visualizations/static/
# - output/visualizations/interactive/
# - output/results/statistical_analysis/
```

## üìà Output File Naming Convention

### Timestamp Format
All files use standardized timestamps: `YYYYMMDD_HHMMSS`
- **YYYY**: 4-digit year
- **MM**: 2-digit month
- **DD**: 2-digit day
- **HH**: 2-digit hour (24-hour format)
- **MM**: 2-digit minute
- **SS**: 2-digit second

### File Categories
- **Analysis Results**: `analysis_type_YYYYMMDD_HHMMSS.csv`
- **Visualizations**: `chart_type_YYYYMMDD_HHMMSS.png/html`
- **Models**: `model_type_YYYYMMDD_HHMMSS.pkl`
- **Reports**: `report_type_YYYYMMDD_HHMMSS.pdf`

### Examples
```
attack_patterns_20231201_143022.csv
security_dashboard_20231201_143022.html
anomaly_model_20231201_143022.pkl
summary_report_20231201_143022.pdf
```

## üîç Monitoring and Logs

### Real-time Log Monitoring
```bash
# Monitor setup process
tail -f output/logs/setup.log

# Monitor data loading
tail -f output/logs/data_loading.log

# Monitor all errors
tail -f output/logs/error.log

# Monitor Hive query performance
tail -f output/logs/hive_queries.log
```

### Log Rotation
Logs are automatically rotated to prevent excessive disk usage:
- **Daily rotation**: Logs older than 7 days are compressed
- **Size limits**: Individual log files are limited to 100MB
- **Cleanup**: Compressed logs older than 30 days are deleted

### Performance Monitoring
```bash
# Check query execution times
grep "Execution time" output/logs/hive_queries.log

# Monitor resource usage
grep "Memory usage" output/logs/performance.log

# Check error rates
grep "ERROR" output/logs/*.log | wc -l
```

## üìä Result Analysis Tools

### CSV Analysis
```python
# Load and analyze query results
import pandas as pd
import glob

# Find latest results
result_files = glob.glob('output/results/query_results/*.csv')
latest_file = max(result_files, key=os.path.getctime)

# Load and analyze
df = pd.read_csv(latest_file)
print(f"Loaded {len(df)} results from {latest_file}")
print(df.describe())
```

### Visualization Review
```python
# Generate visualization index
import os
from datetime import datetime

viz_dir = 'output/visualizations/static'
viz_files = [f for f in os.listdir(viz_dir) if f.endswith('.png')]

print("Available Visualizations:")
for viz_file in sorted(viz_files):
    file_path = os.path.join(viz_dir, viz_file)
    mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))
    print(f"  {viz_file} - Created: {mod_time}")
```

### Report Generation
```python
# Automated report compilation
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter

def generate_summary_report():
    """Generate a summary PDF report from analysis results."""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"output/reports/summary_report_{timestamp}.pdf"
    
    c = canvas.Canvas(filename, pagesize=letter)
    
    # Add title
    c.drawString(100, 750, "UNSW-NB15 Security Analysis Report")
    c.drawString(100, 730, f"Generated: {timestamp}")
    
    # Add analysis results
    # (Implementation would include actual data integration)
    
    c.save()
    return filename
```

## üîß Maintenance and Cleanup

### Automated Cleanup
```bash
#!/bin/bash
# cleanup_outputs.sh - Run weekly to manage disk space

# Remove old log files (older than 30 days)
find output/logs -name "*.log" -mtime +30 -delete

# Compress old results (older than 7 days)
find output/results -name "*.csv" -mtime +7 -exec gzip {} \;

# Remove old visualizations (older than 14 days)
find output/visualizations -name "*.png" -mtime +14 -delete

# Clean up temporary files
find output -name "tmp_*" -delete
find output -name "*.tmp" -delete

echo "Cleanup completed: $(date)"
```

### Manual Cleanup Commands
```bash
# Remove all outputs (WARNING: Destructive)
rm -rf output/results/*
rm -rf output/visualizations/*
rm -rf output/logs/*

# Remove only old files
find output -type f -mtime +7 -delete

# Compress large result files
find output/results -name "*.csv" -size +10M -exec gzip {} \;
```

## üì± Integration with External Tools

### Export to Business Intelligence Tools
```python
# Export results for Tableau, Power BI, etc.
def export_for_bi_tools():
    """Export data in formats suitable for BI tools."""
    
    # Load latest analysis results
    df = pd.read_csv('output/results/latest_analysis.csv')
    
    # Export in multiple formats
    df.to_excel('output/results/bi_export.xlsx', index=False)
    df.to_json('output/results/bi_export.json', orient='records')
    
    # Create Tableau extract format (if tabula-py available)
    try:
        import tableauserverclient as tsc
        # Implementation for Tableau integration
    except ImportError:
        print("Tableau integration not available")
```

### Email Reporting
```python
# Automated email reports (configuration required)
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase

def email_report(recipient, report_file):
    """Send analysis report via email."""
    
    # Email configuration (use environment variables)
    smtp_server = os.getenv('SMTP_SERVER', 'localhost')
    smtp_port = int(os.getenv('SMTP_PORT', 587))
    
    # Create message
    msg = MIMEMultipart()
    msg['Subject'] = f"UNSW-NB15 Analysis Report - {datetime.now().strftime('%Y-%m-%d')}"
    
    # Attach report
    with open(report_file, 'rb') as f:
        attachment = MIMEBase('application', 'octet-stream')
        attachment.set_payload(f.read())
        msg.attach(attachment)
    
    # Send email (implementation depends on your SMTP setup)
```

## üéØ Best Practices

### File Organization
1. **Use timestamps**: Always include timestamps in filenames
2. **Logical grouping**: Separate outputs by type and purpose
3. **Documentation**: Include metadata files explaining results
4. **Version control**: Keep track of analysis versions

### Performance Optimization
1. **Compress large files**: Use gzip for large CSV files
2. **Regular cleanup**: Remove old temporary files
3. **Monitor disk usage**: Set up alerts for disk space
4. **Archive old results**: Move old analyses to long-term storage

### Data Quality
1. **Validate outputs**: Check result files for completeness
2. **Error checking**: Monitor error logs regularly
3. **Backup important results**: Copy critical analyses to backup locations
4. **Document methodology**: Include analysis documentation with results

---

## üìû Support

### Common Issues
- **Disk space errors**: Run cleanup scripts or increase available space
- **Permission errors**: Check file permissions and ownership
- **Missing outputs**: Verify that analysis scripts completed successfully
- **Corrupted files**: Re-run analysis if files appear corrupted

### Monitoring Commands
```bash
# Check disk usage
du -sh output/

# Monitor latest outputs
ls -lt output/results/ | head -10

# Check for errors
grep -r "ERROR" output/logs/

# Verify file integrity
find output -name "*.csv" -exec wc -l {} \; | sort -n
```

---

**Your analytical outputs are valuable!** üìä Make sure to backup important results and maintain good organization practices for reproducible research.