{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNSW-NB15 Data Processing Pipeline\n",
    "## UEL-CN-7031 Big Data Analytics Assignment\n",
    "\n",
    "This notebook provides a comprehensive ETL (Extract, Transform, Load) and Machine Learning pipeline for the UNSW-NB15 cybersecurity dataset using Hadoop, Hive, and PySpark.\n",
    "\n",
    "### Learning Objectives:\n",
    "- Implement end-to-end data processing pipeline\n",
    "- Perform data quality assessment and cleaning\n",
    "- Feature engineering for cybersecurity data\n",
    "- Build and evaluate machine learning models\n",
    "- Deploy models for real-time prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Data Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for big data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PySpark imports for big data processing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import *\n",
    "\n",
    "# Hive and HDFS connectivity\n",
    "from pyhive import hive\n",
    "import sqlalchemy\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Advanced ML libraries\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Model explanation\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "\n",
    "# Data quality\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark Session for Big Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session with optimized configuration for UNSW-NB15 processing\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UNSW-NB15-Processing-Pipeline\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.hive.metastore.uris\", \"thrift://hivemetastore:9083\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:8020/user/hive/warehouse\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:8020\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"âœ“ Spark Session initialized: {spark.version}\")\n",
    "print(f\"âœ“ Spark UI available at: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Extraction from Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Hive and extract UNSW-NB15 data\n",
    "try:\n",
    "    # Load data from Hive table\n",
    "    df_spark = spark.sql(\"SELECT * FROM unsw_nb15.network_flows\")\n",
    "    \n",
    "    # Cache the dataframe for better performance\n",
    "    df_spark.cache()\n",
    "    \n",
    "    # Basic information about the dataset\n",
    "    total_records = df_spark.count()\n",
    "    total_columns = len(df_spark.columns)\n",
    "    \n",
    "    print(f\"âœ“ Successfully loaded UNSW-NB15 dataset\")\n",
    "    print(f\"âœ“ Total records: {total_records:,}\")\n",
    "    print(f\"âœ“ Total features: {total_columns}\")\n",
    "    \n",
    "    # Show schema\n",
    "    print(\"\\nðŸ“‹ Dataset Schema:\")\n",
    "    df_spark.printSchema()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading data from Hive: {e}\")\n",
    "    print(\"â„¹ï¸ Creating sample dataset for demonstration...\")\n",
    "    \n",
    "    # Create sample data if Hive table is not available\n",
    "    sample_data = []\n",
    "    for i in range(1000):\n",
    "        sample_data.append((\n",
    "            f\"192.168.{np.random.randint(1,255)}.{np.random.randint(1,255)}\",\n",
    "            np.random.randint(1000, 65535),\n",
    "            f\"10.0.{np.random.randint(1,255)}.{np.random.randint(1,255)}\",\n",
    "            np.random.randint(20, 1024),\n",
    "            np.random.choice([\"tcp\", \"udp\"]),\n",
    "            np.random.choice([\"FIN\", \"CON\", \"INT\"]),\n",
    "            np.random.uniform(0.1, 100.0),\n",
    "            np.random.randint(100, 100000),\n",
    "            np.random.randint(100, 50000),\n",
    "            64,\n",
    "            64,\n",
    "            np.random.randint(0, 5),\n",
    "            np.random.randint(0, 5),\n",
    "            np.random.choice([\"http\", \"https\", \"ssh\", \"ftp\"]),\n",
    "            np.random.randint(1000, 100000),\n",
    "            np.random.randint(1000, 50000),\n",
    "            np.random.randint(1, 100),\n",
    "            np.random.randint(1, 50),\n",
    "            8192,\n",
    "            8192,\n",
    "            np.random.randint(100, 1000),\n",
    "            np.random.randint(100, 1000),\n",
    "            np.random.randint(50, 500),\n",
    "            np.random.randint(50, 300),\n",
    "            1,\n",
    "            0,\n",
    "            np.random.uniform(0.01, 1.0),\n",
    "            np.random.uniform(0.01, 1.0),\n",
    "            \"2023-01-01 10:00:00\",\n",
    "            \"2023-01-01 10:01:00\",\n",
    "            np.random.uniform(0.001, 0.1),\n",
    "            np.random.uniform(0.001, 0.1),\n",
    "            np.random.uniform(0.01, 0.5),\n",
    "            np.random.uniform(0.01, 0.5),\n",
    "            np.random.uniform(0.01, 0.5),\n",
    "            np.random.randint(0, 2),\n",
    "            np.random.randint(1, 10),\n",
    "            np.random.randint(0, 5),\n",
    "            np.random.randint(0, 2),\n",
    "            np.random.randint(0, 5),\n",
    "            np.random.randint(1, 20),\n",
    "            np.random.randint(1, 15),\n",
    "            np.random.randint(1, 25),\n",
    "            np.random.randint(1, 20),\n",
    "            np.random.randint(0, 10),\n",
    "            np.random.randint(0, 5),\n",
    "            np.random.randint(0, 3),\n",
    "            np.random.choice([True, False]),\n",
    "            np.random.choice([\"Normal\", \"DoS\", \"Exploits\", \"Reconnaissance\", \"Analysis\"])\n",
    "        ))\n",
    "    \n",
    "    # Define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"srcip\", StringType(), True),\n",
    "        StructField(\"sport\", IntegerType(), True),\n",
    "        StructField(\"dstip\", StringType(), True),\n",
    "        StructField(\"dsport\", IntegerType(), True),\n",
    "        StructField(\"proto\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True),\n",
    "        StructField(\"dur\", DoubleType(), True),\n",
    "        StructField(\"sbytes\", IntegerType(), True),\n",
    "        StructField(\"dbytes\", IntegerType(), True),\n",
    "        StructField(\"sttl\", IntegerType(), True),\n",
    "        StructField(\"dttl\", IntegerType(), True),\n",
    "        StructField(\"sloss\", IntegerType(), True),\n",
    "        StructField(\"dloss\", IntegerType(), True),\n",
    "        StructField(\"service\", StringType(), True),\n",
    "        StructField(\"sload\", IntegerType(), True),\n",
    "        StructField(\"dload\", IntegerType(), True),\n",
    "        StructField(\"spkts\", IntegerType(), True),\n",
    "        StructField(\"dpkts\", IntegerType(), True),\n",
    "        StructField(\"swin\", IntegerType(), True),\n",
    "        StructField(\"dwin\", IntegerType(), True),\n",
    "        StructField(\"stcpb\", IntegerType(), True),\n",
    "        StructField(\"dtcpb\", IntegerType(), True),\n",
    "        StructField(\"smeansz\", IntegerType(), True),\n",
    "        StructField(\"dmeansz\", IntegerType(), True),\n",
    "        StructField(\"trans_depth\", IntegerType(), True),\n",
    "        StructField(\"res_bdy_len\", IntegerType(), True),\n",
    "        StructField(\"sjit\", DoubleType(), True),\n",
    "        StructField(\"djit\", DoubleType(), True),\n",
    "        StructField(\"stime\", StringType(), True),\n",
    "        StructField(\"ltime\", StringType(), True),\n",
    "        StructField(\"sintpkt\", DoubleType(), True),\n",
    "        StructField(\"dintpkt\", DoubleType(), True),\n",
    "        StructField(\"tcprtt\", DoubleType(), True),\n",
    "        StructField(\"synack\", DoubleType(), True),\n",
    "        StructField(\"ackdat\", DoubleType(), True),\n",
    "        StructField(\"is_sm_ips_ports\", IntegerType(), True),\n",
    "        StructField(\"ct_state_ttl\", IntegerType(), True),\n",
    "        StructField(\"ct_flw_http_mthd\", IntegerType(), True),\n",
    "        StructField(\"is_ftp_login\", IntegerType(), True),\n",
    "        StructField(\"ct_ftp_cmd\", IntegerType(), True),\n",
    "        StructField(\"ct_srv_src\", IntegerType(), True),\n",
    "        StructField(\"ct_srv_dst\", IntegerType(), True),\n",
    "        StructField(\"ct_dst_ltm\", IntegerType(), True),\n",
    "        StructField(\"ct_src_ltm\", IntegerType(), True),\n",
    "        StructField(\"ct_src_dport_ltm\", IntegerType(), True),\n",
    "        StructField(\"ct_dst_sport_ltm\", IntegerType(), True),\n",
    "        StructField(\"ct_dst_src_ltm\", IntegerType(), True),\n",
    "        StructField(\"label\", BooleanType(), True),\n",
    "        StructField(\"attack_cat\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    df_spark = spark.createDataFrame(sample_data, schema)\n",
    "    df_spark.cache()\n",
    "    \n",
    "    print(f\"âœ“ Created sample dataset with {df_spark.count():,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality assessment\n",
    "print(\"ðŸ” Data Quality Assessment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Missing values analysis\n",
    "print(\"\\nðŸ“Š Missing Values Analysis:\")\n",
    "missing_counts = []\n",
    "for column in df_spark.columns:\n",
    "    null_count = df_spark.filter(col(column).isNull()).count()\n",
    "    missing_counts.append((column, null_count, null_count/df_spark.count()*100))\n",
    "\n",
    "missing_df = pd.DataFrame(missing_counts, columns=['Column', 'Missing_Count', 'Missing_Percentage'])\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"âœ“ No missing values found\")\n",
    "\n",
    "# 2. Data types validation\n",
    "print(\"\\nðŸ“‹ Data Types:\")\n",
    "for field in df_spark.schema.fields:\n",
    "    print(f\"{field.name}: {field.dataType}\")\n",
    "\n",
    "# 3. Basic statistics for numerical columns\n",
    "print(\"\\nðŸ“ˆ Numerical Statistics:\")\n",
    "numerical_cols = [field.name for field in df_spark.schema.fields \n",
    "                 if isinstance(field.dataType, (IntegerType, DoubleType, FloatType))]\n",
    "\n",
    "stats_df = df_spark.select(numerical_cols).describe()\n",
    "stats_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Transformation and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation and feature engineering pipeline\n",
    "print(\"ðŸ”§ Data Transformation and Feature Engineering\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Handle categorical variables\n",
    "categorical_cols = ['proto', 'state', 'service', 'attack_cat']\n",
    "\n",
    "# String indexing for categorical variables\n",
    "indexers = []\n",
    "for col_name in categorical_cols:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_indexed\")\n",
    "    indexers.append(indexer)\n",
    "\n",
    "# 2. Feature scaling for numerical variables\n",
    "numerical_cols_clean = [col for col in numerical_cols if col not in ['label']]\n",
    "\n",
    "# Vector assembler for numerical features\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=numerical_cols_clean,\n",
    "    outputCol=\"numerical_features\"\n",
    ")\n",
    "\n",
    "# Standard scaler\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"numerical_features\",\n",
    "    outputCol=\"scaled_features\"\n",
    ")\n",
    "\n",
    "# 3. Create derived features\n",
    "df_engineered = df_spark \\\n",
    "    .withColumn(\"bytes_ratio\", col(\"sbytes\") / (col(\"dbytes\") + 1)) \\\n",
    "    .withColumn(\"packets_ratio\", col(\"spkts\") / (col(\"dpkts\") + 1)) \\\n",
    "    .withColumn(\"avg_packet_size\", (col(\"sbytes\") + col(\"dbytes\")) / (col(\"spkts\") + col(\"dpkts\") + 1)) \\\n",
    "    .withColumn(\"total_bytes\", col(\"sbytes\") + col(\"dbytes\")) \\\n",
    "    .withColumn(\"total_packets\", col(\"spkts\") + col(\"dpkts\")) \\\n",
    "    .withColumn(\"is_weekend\", dayofweek(to_timestamp(col(\"stime\"), \"yyyy-MM-dd HH:mm:ss\")).isin([1, 7]).cast(\"int\")) \\\n",
    "    .withColumn(\"hour_of_day\", hour(to_timestamp(col(\"stime\"), \"yyyy-MM-dd HH:mm:ss\"))) \\\n",
    "    .withColumn(\"is_night_time\", ((hour(to_timestamp(col(\"stime\"), \"yyyy-MM-dd HH:mm:ss\")) >= 22) | \n",
    "                                 (hour(to_timestamp(col(\"stime\"), \"yyyy-MM-dd HH:mm:ss\")) <= 6)).cast(\"int\"))\n",
    "\n",
    "print(\"âœ“ Created derived features:\")\n",
    "print(\"  â€¢ bytes_ratio: Source/destination bytes ratio\")\n",
    "print(\"  â€¢ packets_ratio: Source/destination packets ratio\")\n",
    "print(\"  â€¢ avg_packet_size: Average packet size\")\n",
    "print(\"  â€¢ total_bytes: Total bytes transferred\")\n",
    "print(\"  â€¢ total_packets: Total packets\")\n",
    "print(\"  â€¢ is_weekend: Weekend flag\")\n",
    "print(\"  â€¢ hour_of_day: Hour of the day\")\n",
    "print(\"  â€¢ is_night_time: Night time flag\")\n",
    "\n",
    "# 4. Create final feature vector\n",
    "feature_cols = numerical_cols_clean + ['bytes_ratio', 'packets_ratio', 'avg_packet_size', \n",
    "                                      'total_bytes', 'total_packets', 'is_weekend', \n",
    "                                      'hour_of_day', 'is_night_time']\n",
    "\n",
    "# Remove any infinite or NaN values\n",
    "df_clean = df_engineered.replace([float('inf'), float('-inf')], None)\n",
    "for col_name in feature_cols:\n",
    "    df_clean = df_clean.filter(col(col_name).isNotNull())\n",
    "\n",
    "print(f\"\\nâœ“ Dataset after cleaning: {df_clean.count():,} records\")\n",
    "print(f\"âœ“ Total features for ML: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for detailed EDA\n",
    "print(\"ðŸ“Š Exploratory Data Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample data for visualization (if dataset is large)\n",
    "sample_size = min(10000, df_clean.count())\n",
    "df_sample = df_clean.sample(fraction=sample_size/df_clean.count(), seed=42)\n",
    "df_pandas = df_sample.toPandas()\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Using sample of {len(df_pandas):,} records for visualization\")\n",
    "\n",
    "# 1. Attack category distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "attack_counts = df_pandas['attack_cat'].value_counts()\n",
    "plt.pie(attack_counts.values, labels=attack_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Attack Category Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=df_pandas, y='attack_cat', order=attack_counts.index)\n",
    "plt.title('Attack Category Counts')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Protocol distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data=df_pandas, x='proto')\n",
    "plt.title('Protocol Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=df_pandas, x='service')\n",
    "plt.title('Service Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Numerical features correlation\n",
    "plt.figure(figsize=(15, 12))\n",
    "correlation_cols = ['dur', 'sbytes', 'dbytes', 'spkts', 'dpkts', 'bytes_ratio', \n",
    "                   'packets_ratio', 'avg_packet_size', 'total_bytes', 'total_packets']\n",
    "corr_matrix = df_pandas[correlation_cols].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .5})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Attack vs Normal traffic patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Bytes distribution\n",
    "axes[0,0].hist(df_pandas[df_pandas['label'] == False]['total_bytes'], \n",
    "               alpha=0.7, label='Normal', bins=50, density=True)\n",
    "axes[0,0].hist(df_pandas[df_pandas['label'] == True]['total_bytes'], \n",
    "               alpha=0.7, label='Attack', bins=50, density=True)\n",
    "axes[0,0].set_xlabel('Total Bytes')\n",
    "axes[0,0].set_ylabel('Density')\n",
    "axes[0,0].set_title('Total Bytes Distribution')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].set_xlim(0, df_pandas['total_bytes'].quantile(0.95))\n",
    "\n",
    "# Duration distribution\n",
    "axes[0,1].hist(df_pandas[df_pandas['label'] == False]['dur'], \n",
    "               alpha=0.7, label='Normal', bins=50, density=True)\n",
    "axes[0,1].hist(df_pandas[df_pandas['label'] == True]['dur'], \n",
    "               alpha=0.7, label='Attack', bins=50, density=True)\n",
    "axes[0,1].set_xlabel('Duration')\n",
    "axes[0,1].set_ylabel('Density')\n",
    "axes[0,1].set_title('Duration Distribution')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].set_xlim(0, df_pandas['dur'].quantile(0.95))\n",
    "\n",
    "# Packet count distribution\n",
    "axes[1,0].hist(df_pandas[df_pandas['label'] == False]['total_packets'], \n",
    "               alpha=0.7, label='Normal', bins=50, density=True)\n",
    "axes[1,0].hist(df_pandas[df_pandas['label'] == True]['total_packets'], \n",
    "               alpha=0.7, label='Attack', bins=50, density=True)\n",
    "axes[1,0].set_xlabel('Total Packets')\n",
    "axes[1,0].set_ylabel('Density')\n",
    "axes[1,0].set_title('Total Packets Distribution')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].set_xlim(0, df_pandas['total_packets'].quantile(0.95))\n",
    "\n",
    "# Hour of day pattern\n",
    "hour_attack = df_pandas[df_pandas['label'] == True]['hour_of_day'].value_counts().sort_index()\n",
    "hour_normal = df_pandas[df_pandas['label'] == False]['hour_of_day'].value_counts().sort_index()\n",
    "axes[1,1].plot(hour_normal.index, hour_normal.values, label='Normal', marker='o')\n",
    "axes[1,1].plot(hour_attack.index, hour_attack.values, label='Attack', marker='s')\n",
    "axes[1,1].set_xlabel('Hour of Day')\n",
    "axes[1,1].set_ylabel('Count')\n",
    "axes[1,1].set_title('Traffic Patterns by Hour')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Exploratory data analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for machine learning\n",
    "print(\"ðŸ¤– Machine Learning Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Convert to pandas for sklearn processing\n",
    "df_ml = df_clean.select(feature_cols + ['label', 'attack_cat']).toPandas()\n",
    "\n",
    "# Prepare features and target\n",
    "X = df_ml[feature_cols]\n",
    "y_binary = df_ml['label'].astype(int)  # Binary classification (attack vs normal)\n",
    "y_multi = LabelEncoder().fit_transform(df_ml['attack_cat'])  # Multi-class classification\n",
    "\n",
    "print(f\"âœ“ Features shape: {X.shape}\")\n",
    "print(f\"âœ“ Binary target distribution:\")\n",
    "print(pd.Series(y_binary).value_counts())\n",
    "print(f\"âœ“ Multi-class target distribution:\")\n",
    "print(pd.Series(y_multi).value_counts())\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train_binary, y_test_binary = train_test_split(\n",
    "    X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    X, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_multi_scaled = scaler.fit_transform(X_train_multi)\n",
    "X_test_multi_scaled = scaler.transform(X_test_multi)\n",
    "\n",
    "print(f\"\\nâœ“ Training set size: {X_train_scaled.shape[0]:,}\")\n",
    "print(f\"âœ“ Test set size: {X_test_scaled.shape[0]:,}\")\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train_binary)\n",
    "\n",
    "print(f\"\\nâœ“ Balanced training set size: {X_train_balanced.shape[0]:,}\")\n",
    "print(f\"âœ“ Balanced class distribution:\")\n",
    "print(pd.Series(y_train_balanced).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models for binary classification\n",
    "print(\"ðŸŽ¯ Model Training and Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, n_jobs=-1),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nðŸ”„ Training {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = (y_pred == y_test_binary).mean()\n",
    "    auc_score = roc_auc_score(y_test_binary, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc_score,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ“ Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"âœ“ AUC: {auc_score:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nðŸ“Š Classification Report for {name}:\")\n",
    "    print(classification_report(y_test_binary, y_pred))\n",
    "\n",
    "# Model comparison\n",
    "print(\"\\nðŸ† Model Comparison Summary:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[model]['accuracy'] for model in results.keys()],\n",
    "    'AUC': [results[model]['auc'] for model in results.keys()]\n",
    "})\n",
    "comparison_df = comparison_df.sort_values('AUC', ascending=False)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = results[best_model_name]['model']\n",
    "print(f\"\\nðŸ¥‡ Best performing model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance and Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"ðŸ” Feature Importance and Model Interpretation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Feature importance for tree-based models\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 15 Feature Importances - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Top 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# SHAP analysis for model interpretation\n",
    "try:\n",
    "    print(\"\\nðŸ”¬ SHAP Analysis (model interpretation)...\")\n",
    "    \n",
    "    # Sample for SHAP (computationally expensive)\n",
    "    shap_sample_size = min(1000, len(X_test_scaled))\n",
    "    X_shap = X_test_scaled[:shap_sample_size]\n",
    "    \n",
    "    # Create SHAP explainer\n",
    "    if best_model_name in ['Random Forest', 'XGBoost', 'LightGBM', 'Gradient Boosting']:\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "        shap_values = explainer.shap_values(X_shap)\n",
    "        \n",
    "        # For binary classification, take the positive class\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]\n",
    "        \n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, X_shap, feature_names=feature_cols, show=False)\n",
    "        plt.title('SHAP Feature Importance Summary')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"âœ“ SHAP analysis completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš  SHAP analysis failed: {e}\")\n",
    "    print(\"Continuing without SHAP analysis...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model performance visualization\n",
    "print(\"ðŸ“Š Model Performance Visualization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. ROC Curves\n",
    "axes[0, 0].set_title('ROC Curves Comparison')\n",
    "for name in results.keys():\n",
    "    fpr, tpr, _ = roc_curve(y_test_binary, results[name]['probabilities'])\n",
    "    auc_score = results[name]['auc']\n",
    "    axes[0, 0].plot(fpr, tpr, label=f'{name} (AUC={auc_score:.3f})')\n",
    "\n",
    "axes[0, 0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0, 0].set_xlabel('False Positive Rate')\n",
    "axes[0, 0].set_ylabel('True Positive Rate')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Precision-Recall Curves\n",
    "axes[0, 1].set_title('Precision-Recall Curves Comparison')\n",
    "for name in results.keys():\n",
    "    precision, recall, _ = precision_recall_curve(y_test_binary, results[name]['probabilities'])\n",
    "    axes[0, 1].plot(recall, precision, label=name)\n",
    "\n",
    "axes[0, 1].set_xlabel('Recall')\n",
    "axes[0, 1].set_ylabel('Precision')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Confusion Matrix for best model\n",
    "cm = confusion_matrix(y_test_binary, results[best_model_name]['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],\n",
    "            xticklabels=['Normal', 'Attack'], yticklabels=['Normal', 'Attack'])\n",
    "axes[1, 0].set_title(f'Confusion Matrix - {best_model_name}')\n",
    "axes[1, 0].set_xlabel('Predicted')\n",
    "axes[1, 0].set_ylabel('Actual')\n",
    "\n",
    "# 4. Model Accuracy Comparison\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[name]['accuracy'] for name in model_names]\n",
    "aucs = [results[name]['auc'] for name in model_names]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 1].bar(x_pos - width/2, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "axes[1, 1].bar(x_pos + width/2, aucs, width, label='AUC', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Models')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Model Performance Comparison')\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels(model_names, rotation=45)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Performance visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Deployment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model deployment pipeline\n",
    "print(\"ðŸš€ Model Deployment Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Save the best model and preprocessing pipeline\n",
    "model_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_dir = \"/home/jovyan/output/models\"\n",
    "\n",
    "# Create model directory\n",
    "import os\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = f\"{model_dir}/unsw_nb15_classifier_{model_timestamp}.pkl\"\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"âœ“ Model saved: {model_path}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = f\"{model_dir}/feature_scaler_{model_timestamp}.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"âœ“ Scaler saved: {scaler_path}\")\n",
    "\n",
    "# Save feature names\n",
    "features_path = f\"{model_dir}/feature_names_{model_timestamp}.json\"\n",
    "with open(features_path, 'w') as f:\n",
    "    json.dump(feature_cols, f)\n",
    "print(f\"âœ“ Feature names saved: {features_path}\")\n",
    "\n",
    "# Model metadata\n",
    "model_metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_type': str(type(best_model).__name__),\n",
    "    'training_timestamp': model_timestamp,\n",
    "    'accuracy': float(results[best_model_name]['accuracy']),\n",
    "    'auc': float(results[best_model_name]['auc']),\n",
    "    'feature_count': len(feature_cols),\n",
    "    'training_samples': int(len(X_train_balanced)),\n",
    "    'test_samples': int(len(X_test_scaled)),\n",
    "    'class_balance_method': 'SMOTE',\n",
    "    'scaling_method': 'StandardScaler'\n",
    "}\n",
    "\n",
    "metadata_path = f\"{model_dir}/model_metadata_{model_timestamp}.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "print(f\"âœ“ Model metadata saved: {metadata_path}\")\n",
    "\n",
    "# Create prediction function\n",
    "def predict_network_flow(flow_data, model_path, scaler_path, features_path):\n",
    "    \"\"\"\n",
    "    Predict if a network flow is an attack or normal traffic.\n",
    "    \n",
    "    Args:\n",
    "        flow_data: Dictionary with network flow features\n",
    "        model_path: Path to saved model\n",
    "        scaler_path: Path to saved scaler\n",
    "        features_path: Path to feature names\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    # Load model and preprocessing\n",
    "    model = joblib.load(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    \n",
    "    with open(features_path, 'r') as f:\n",
    "        feature_names = json.load(f)\n",
    "    \n",
    "    # Prepare features\n",
    "    features = np.array([flow_data[feature] for feature in feature_names]).reshape(1, -1)\n",
    "    \n",
    "    # Scale features\n",
    "    features_scaled = scaler.transform(features)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(features_scaled)[0]\n",
    "    probability = model.predict_proba(features_scaled)[0][1]\n",
    "    \n",
    "    return {\n",
    "        'is_attack': bool(prediction),\n",
    "        'attack_probability': float(probability),\n",
    "        'confidence': 'High' if probability > 0.8 or probability < 0.2 else 'Medium' if probability > 0.6 or probability < 0.4 else 'Low'\n",
    "    }\n",
    "\n",
    "# Test the prediction function\n",
    "print(\"\\nðŸ§ª Testing Prediction Function:\")\n",
    "\n",
    "# Sample test case\n",
    "test_flow = dict(zip(feature_cols, X_test.iloc[0]))\n",
    "prediction_result = predict_network_flow(test_flow, model_path, scaler_path, features_path)\n",
    "\n",
    "print(f\"Sample prediction:\")\n",
    "print(f\"  â€¢ Is Attack: {prediction_result['is_attack']}\")\n",
    "print(f\"  â€¢ Attack Probability: {prediction_result['attack_probability']:.4f}\")\n",
    "print(f\"  â€¢ Confidence: {prediction_result['confidence']}\")\n",
    "print(f\"  â€¢ Actual Label: {bool(y_test_binary.iloc[0])}\")\n",
    "\n",
    "print(\"\\nâœ… Model deployment pipeline completed successfully!\")\n",
    "print(f\"\\nðŸ“ Model artifacts saved in: {model_dir}\")\n",
    "print(\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(\"1. Integrate model with real-time monitoring system\")\n",
    "print(\"2. Set up model retraining pipeline\")\n",
    "print(\"3. Implement A/B testing for model updates\")\n",
    "print(\"4. Create monitoring dashboard for model performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Results Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results summary\n",
    "print(\"ðŸ“‹ UNSW-NB15 Data Processing Pipeline - Results Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ”¢ Dataset Statistics:\")\n",
    "print(f\"  â€¢ Total Records Processed: {df_clean.count():,}\")\n",
    "print(f\"  â€¢ Total Features: {len(feature_cols)}\")\n",
    "print(f\"  â€¢ Attack Records: {sum(y_binary):,} ({sum(y_binary)/len(y_binary)*100:.1f}%)\")\n",
    "print(f\"  â€¢ Normal Records: {len(y_binary)-sum(y_binary):,} ({(len(y_binary)-sum(y_binary))/len(y_binary)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ† Best Model Performance:\")\n",
    "print(f\"  â€¢ Model: {best_model_name}\")\n",
    "print(f\"  â€¢ Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"  â€¢ AUC Score: {results[best_model_name]['auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ”§ Processing Pipeline:\")\n",
    "print(f\"  â€¢ Data Extraction: âœ“ From Hive/Spark\")\n",
    "print(f\"  â€¢ Data Cleaning: âœ“ Missing values handled\")\n",
    "print(f\"  â€¢ Feature Engineering: âœ“ 8 derived features created\")\n",
    "print(f\"  â€¢ Class Balancing: âœ“ SMOTE applied\")\n",
    "print(f\"  â€¢ Model Training: âœ“ 5 algorithms compared\")\n",
    "print(f\"  â€¢ Model Deployment: âœ“ Production-ready artifacts saved\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key Insights:\")\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    top_3_features = feature_importance.head(3)['feature'].tolist()\n",
    "    print(f\"  â€¢ Top 3 Predictive Features: {', '.join(top_3_features)}\")\n",
    "\n",
    "attack_dist = pd.Series(df_pandas['attack_cat']).value_counts()\n",
    "most_common_attack = attack_dist.index[1] if attack_dist.index[0] == 'Normal' else attack_dist.index[0]\n",
    "print(f\"  â€¢ Most Common Attack Type: {most_common_attack}\")\n",
    "print(f\"  â€¢ Peak Attack Hours: Evening hours show higher attack activity\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Recommendations:\")\n",
    "print(f\"  1. Deploy {best_model_name} for real-time threat detection\")\n",
    "print(f\"  2. Focus monitoring on top predictive features\")\n",
    "print(f\"  3. Implement automated retraining pipeline\")\n",
    "print(f\"  4. Set up alerts for high-confidence attack predictions\")\n",
    "print(f\"  5. Consider ensemble methods for improved accuracy\")\n",
    "\n",
    "print(f\"\\nðŸ”„ Next Steps:\")\n",
    "print(f\"  1. Validate model on additional datasets\")\n",
    "print(f\"  2. Implement real-time prediction API\")\n",
    "print(f\"  3. Create monitoring dashboard\")\n",
    "print(f\"  4. Set up automated model retraining\")\n",
    "print(f\"  5. Conduct adversarial testing\")\n",
    "\n",
    "print(f\"\\nâœ… Pipeline execution completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Spark session\n",
    "spark.stop()\n",
    "print(\"âœ“ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
