{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Cybersecurity\n",
    "## UNSW-NB15 Attack Classification and Anomaly Detection\n",
    "\n",
    "This notebook demonstrates advanced machine learning techniques for cybersecurity analytics using the UNSW-NB15 dataset.\n",
    "\n",
    "### Learning Objectives:\n",
    "- Implement classification models for attack detection\n",
    "- Compare different anomaly detection algorithms\n",
    "- Evaluate model performance and interpret results\n",
    "- Understand feature importance in cybersecurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (use sample data for demonstration)\n",
    "def load_ml_data():\n",
    "    \"\"\"\n",
    "    Load and preprocess data for machine learning\n",
    "    \"\"\"\n",
    "    # For demonstration, generate realistic sample data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 5000\n",
    "    \n",
    "    # Generate features similar to UNSW-NB15\n",
    "    data = {\n",
    "        'dur': np.random.exponential(2, n_samples),\n",
    "        'sbytes': np.random.lognormal(8, 2, n_samples),\n",
    "        'dbytes': np.random.lognormal(7, 2, n_samples),\n",
    "        'sttl': np.random.choice([32, 64, 128, 255], n_samples),\n",
    "        'dttl': np.random.choice([32, 64, 128, 255], n_samples),\n",
    "        'spkts': np.random.poisson(10, n_samples),\n",
    "        'dpkts': np.random.poisson(8, n_samples),\n",
    "        'sload': np.random.lognormal(10, 3, n_samples),\n",
    "        'dload': np.random.lognormal(9, 3, n_samples),\n",
    "        'sintpkt': np.random.exponential(0.1, n_samples),\n",
    "        'dintpkt': np.random.exponential(0.1, n_samples),\n",
    "        'tcprtt': np.random.exponential(0.05, n_samples),\n",
    "        'synack': np.random.exponential(0.02, n_samples),\n",
    "        'ackdat': np.random.exponential(0.02, n_samples)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add categorical features\n",
    "    protocols = ['tcp', 'udp', 'icmp']\n",
    "    services = ['http', 'https', 'ftp', 'ssh', 'dns', '-']\n",
    "    attack_categories = ['Normal', 'DoS', 'Exploits', 'Reconnaissance', 'Analysis', 'Backdoor']\n",
    "    \n",
    "    df['proto'] = np.random.choice(protocols, n_samples, p=[0.8, 0.15, 0.05])\n",
    "    df['service'] = np.random.choice(services, n_samples, p=[0.3, 0.2, 0.1, 0.1, 0.1, 0.2])\n",
    "    df['attack_cat'] = np.random.choice(attack_categories, n_samples, \n",
    "                                       p=[0.7, 0.1, 0.08, 0.05, 0.04, 0.03])\n",
    "    \n",
    "    # Create binary label\n",
    "    df['label'] = (df['attack_cat'] != 'Normal').astype(int)\n",
    "    \n",
    "    # Add derived features\n",
    "    df['total_bytes'] = df['sbytes'] + df['dbytes']\n",
    "    df['total_pkts'] = df['spkts'] + df['dpkts']\n",
    "    df['bytes_per_pkt'] = df['total_bytes'] / (df['total_pkts'] + 1)\n",
    "    df['duration_per_byte'] = df['dur'] / (df['total_bytes'] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "df = load_ml_data()\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Attack distribution: {df['label'].value_counts()}\")\n",
    "print(f\"\\nAttack categories: {df['attack_cat'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering and preprocessing\n",
    "def preprocess_features(df):\n",
    "    \"\"\"\n",
    "    Preprocess features for machine learning\n",
    "    \"\"\"\n",
    "    # Select numerical features\n",
    "    numerical_features = ['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'spkts', 'dpkts',\n",
    "                         'sload', 'dload', 'sintpkt', 'dintpkt', 'tcprtt', 'synack', 'ackdat',\n",
    "                         'total_bytes', 'total_pkts', 'bytes_per_pkt', 'duration_per_byte']\n",
    "    \n",
    "    # Handle categorical features\n",
    "    categorical_features = ['proto', 'service']\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X_num = df[numerical_features].fillna(0)\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    X_cat = pd.get_dummies(df[categorical_features], prefix=categorical_features)\n",
    "    \n",
    "    # Combine features\n",
    "    X = pd.concat([X_num, X_cat], axis=1)\n",
    "    \n",
    "    # Target variables\n",
    "    y_binary = df['label']  # Binary classification\n",
    "    y_multi = df['attack_cat']  # Multi-class classification\n",
    "    \n",
    "    return X, y_binary, y_multi, numerical_features\n",
    "\n",
    "X, y_binary, y_multi, numerical_features = preprocess_features(df)\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Features: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Binary Classification: Attack vs Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for binary classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42, stratify=y_binary)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")\n",
    "print(f\"Training set attack rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test set attack rate: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "results = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train classifier\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    y_pred_proba = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': clf,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba,\n",
    "        'auc': auc_score\n",
    "    }\n",
    "    \n",
    "    print(f\"AUC Score: {auc_score:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Normal', 'Attack']))\n",
    "\n",
    "print(\"\\nModel comparison:\")\n",
    "for name, result in results.items():\n",
    "    print(f\"{name}: AUC = {result['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ROC curves\n",
    "plt.subplot(1, 2, 1)\n",
    "for name, result in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['probabilities'])\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {result['auc']:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion matrix for best model\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['auc'])\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Normal', 'Attack'], \n",
    "           yticklabels=['Normal', 'Attack'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best performing model: {best_model_name} (AUC = {results[best_model_name]['auc']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest\n",
    "rf_model = results['Random Forest']['model']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='skyblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Most Important Features (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(top_features['importance']):\n",
    "    plt.text(v + 0.001, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 most important features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-class Classification: Attack Type Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for multi-class classification\n",
    "# Only use attack data for attack type classification\n",
    "attack_mask = y_binary == 1\n",
    "X_attacks = X[attack_mask]\n",
    "y_attacks = y_multi[attack_mask]\n",
    "\n",
    "# Remove 'Normal' category\n",
    "y_attacks = y_attacks[y_attacks != 'Normal']\n",
    "X_attacks = X_attacks[y_attacks.index]\n",
    "\n",
    "print(f\"Attack samples for multi-class classification: {len(X_attacks)}\")\n",
    "print(f\"Attack type distribution:\\n{y_attacks.value_counts()}\")\n",
    "\n",
    "# Encode attack categories\n",
    "le = LabelEncoder()\n",
    "y_attacks_encoded = le.fit_transform(y_attacks)\n",
    "\n",
    "# Split data\n",
    "X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(\n",
    "    X_attacks, y_attacks_encoded, test_size=0.3, random_state=42, stratify=y_attacks_encoded\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_mc = StandardScaler()\n",
    "X_train_mc_scaled = scaler_mc.fit_transform(X_train_mc)\n",
    "X_test_mc_scaled = scaler_mc.transform(X_test_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multi-class classifier\n",
    "rf_multiclass = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_multiclass.fit(X_train_mc_scaled, y_train_mc)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_mc = rf_multiclass.predict(X_test_mc_scaled)\n",
    "\n",
    "# Classification report\n",
    "print(\"Multi-class Classification Report:\")\n",
    "print(classification_report(y_test_mc, y_pred_mc, target_names=le.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm_mc = confusion_matrix(y_test_mc, y_pred_mc)\n",
    "sns.heatmap(cm_mc, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title('Multi-class Attack Type Classification')\n",
    "plt.ylabel('Actual Attack Type')\n",
    "plt.xlabel('Predicted Attack Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for anomaly detection (use only numerical features for simplicity)\n",
    "X_num = df[numerical_features].fillna(0)\n",
    "\n",
    "# Scale the data\n",
    "scaler_anom = StandardScaler()\n",
    "X_num_scaled = scaler_anom.fit_transform(X_num)\n",
    "\n",
    "# Apply PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_num_scaled)\n",
    "\n",
    "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest anomaly detection\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "anomaly_labels_iso = iso_forest.fit_predict(X_num_scaled)\n",
    "anomaly_scores_iso = iso_forest.decision_function(X_num_scaled)\n",
    "\n",
    "# DBSCAN clustering (outliers are labeled as -1)\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "cluster_labels = dbscan.fit_predict(X_pca)  # Use PCA for visualization\n",
    "anomaly_labels_dbscan = (cluster_labels == -1).astype(int)\n",
    "\n",
    "# Statistical anomaly detection (Z-score based)\n",
    "z_scores = np.abs((X_num_scaled - X_num_scaled.mean(axis=0)) / X_num_scaled.std(axis=0))\n",
    "anomaly_labels_zscore = (z_scores.max(axis=1) > 3).astype(int)\n",
    "\n",
    "print(f\"Isolation Forest anomalies: {(anomaly_labels_iso == -1).sum()} ({(anomaly_labels_iso == -1).mean():.2%})\")\n",
    "print(f\"DBSCAN outliers: {anomaly_labels_dbscan.sum()} ({anomaly_labels_dbscan.mean():.2%})\")\n",
    "print(f\"Z-score anomalies: {anomaly_labels_zscore.sum()} ({anomaly_labels_zscore.mean():.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anomaly detection results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Original data with true labels\n",
    "axes[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=df['label'], cmap='coolwarm', alpha=0.7)\n",
    "axes[0, 0].set_title('True Labels (Red=Attack, Blue=Normal)')\n",
    "axes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "axes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "\n",
    "# Isolation Forest results\n",
    "iso_colors = ['red' if x == -1 else 'blue' for x in anomaly_labels_iso]\n",
    "axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=iso_colors, alpha=0.7)\n",
    "axes[0, 1].set_title('Isolation Forest Anomalies')\n",
    "axes[0, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "axes[0, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "\n",
    "# DBSCAN results\n",
    "dbscan_colors = ['red' if x == -1 else 'blue' for x in cluster_labels]\n",
    "axes[1, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_colors, alpha=0.7)\n",
    "axes[1, 0].set_title('DBSCAN Outliers')\n",
    "axes[1, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "axes[1, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "\n",
    "# Z-score results\n",
    "zscore_colors = ['red' if x == 1 else 'blue' for x in anomaly_labels_zscore]\n",
    "axes[1, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=zscore_colors, alpha=0.7)\n",
    "axes[1, 1].set_title('Z-score Anomalies (>3σ)')\n",
    "axes[1, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "axes[1, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate anomaly detection performance\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Convert Isolation Forest labels to binary (1 for anomaly)\n",
    "iso_binary = (anomaly_labels_iso == -1).astype(int)\n",
    "\n",
    "anomaly_methods = {\n",
    "    'Isolation Forest': iso_binary,\n",
    "    'DBSCAN': anomaly_labels_dbscan,\n",
    "    'Z-score': anomaly_labels_zscore\n",
    "}\n",
    "\n",
    "print(\"Anomaly Detection Performance (using true attack labels):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "for method_name, predictions in anomaly_methods.items():\n",
    "    precision = precision_score(df['label'], predictions)\n",
    "    recall = recall_score(df['label'], predictions)\n",
    "    f1 = f1_score(df['label'], predictions)\n",
    "    \n",
    "    performance_results.append({\n",
    "        'Method': method_name,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"{method_name}:\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall: {recall:.3f}\")\n",
    "    print(f\"  F1-Score: {f1:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Create performance comparison dataframe\n",
    "perf_df = pd.DataFrame(performance_results)\n",
    "print(\"Performance Summary:\")\n",
    "print(perf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Interpretation and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of misclassified samples\n",
    "best_model = results['Random Forest']['model']\n",
    "y_pred_best = results['Random Forest']['predictions']\n",
    "\n",
    "# Find misclassified samples\n",
    "misclassified = y_test != y_pred_best\n",
    "false_positives = (y_test == 0) & (y_pred_best == 1)\n",
    "false_negatives = (y_test == 1) & (y_pred_best == 0)\n",
    "\n",
    "print(f\"Total misclassified: {misclassified.sum()} ({misclassified.mean():.2%})\")\n",
    "print(f\"False positives: {false_positives.sum()} ({false_positives.mean():.2%})\")\n",
    "print(f\"False negatives: {false_negatives.sum()} ({false_negatives.mean():.2%})\")\n",
    "\n",
    "# Analyze characteristics of misclassified samples\n",
    "X_test_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "misclassified_features = X_test_df[misclassified][numerical_features].describe()\n",
    "\n",
    "print(\"\\nCharacteristics of misclassified samples:\")\n",
    "print(misclassified_features[['mean', 'std']].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation with prediction confidence\n",
    "confidence_scores = np.max(results['Random Forest']['model'].predict_proba(X_test_scaled), axis=1)\n",
    "X_test_df['confidence'] = confidence_scores\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = []\n",
    "for feature in numerical_features:\n",
    "    corr = X_test_df[feature].corr(X_test_df['confidence'])\n",
    "    correlations.append({'feature': feature, 'correlation': corr})\n",
    "\n",
    "corr_df = pd.DataFrame(correlations).sort_values('correlation', key=abs, ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(corr_df)), corr_df['correlation'], color='lightblue')\n",
    "plt.yticks(range(len(corr_df)), corr_df['feature'])\n",
    "plt.xlabel('Correlation with Prediction Confidence')\n",
    "plt.title('Feature Correlation with Model Confidence')\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "for i, v in enumerate(corr_df['correlation']):\n",
    "    plt.text(v + 0.01 if v >= 0 else v - 0.01, i, f'{v:.3f}', \n",
    "             va='center', ha='left' if v >= 0 else 'right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Features most correlated with prediction confidence:\")\n",
    "print(corr_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Cybersecurity Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate practical insights for cybersecurity\n",
    "print(\"=\" * 60)\n",
    "print(\"CYBERSECURITY MACHINE LEARNING INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Model Performance Summary\n",
    "print(\"\\n1. MODEL PERFORMANCE SUMMARY:\")\n",
    "print(\"-\" * 30)\n",
    "best_auc = max(result['auc'] for result in results.values())\n",
    "print(f\"• Best binary classifier: {best_model_name} (AUC = {best_auc:.3f})\")\n",
    "print(f\"• Multi-class attack type accuracy: {(y_test_mc == y_pred_mc).mean():.3f}\")\n",
    "print(f\"• Best anomaly detection method: {perf_df.loc[perf_df['F1-Score'].idxmax(), 'Method']}\")\n",
    "\n",
    "# 2. Feature Importance Insights\n",
    "print(\"\\n2. CRITICAL FEATURES FOR DETECTION:\")\n",
    "print(\"-\" * 30)\n",
    "top_3_features = feature_importance.head(3)\n",
    "for idx, row in top_3_features.iterrows():\n",
    "    print(f\"• {row['feature']}: {row['importance']:.3f} importance\")\n",
    "\n",
    "# 3. Attack Pattern Analysis\n",
    "print(\"\\n3. ATTACK PATTERN ANALYSIS:\")\n",
    "print(\"-\" * 30)\n",
    "attack_stats = df[df['label'] == 1][numerical_features].describe()\n",
    "normal_stats = df[df['label'] == 0][numerical_features].describe()\n",
    "\n",
    "# Find features with biggest differences\n",
    "differences = []\n",
    "for feature in numerical_features:\n",
    "    attack_mean = attack_stats.loc['mean', feature]\n",
    "    normal_mean = normal_stats.loc['mean', feature]\n",
    "    ratio = attack_mean / (normal_mean + 1e-6)  # Avoid division by zero\n",
    "    differences.append({'feature': feature, 'ratio': ratio})\n",
    "\n",
    "diff_df = pd.DataFrame(differences).sort_values('ratio', ascending=False)\n",
    "print(f\"• Features with highest attack/normal ratio:\")\n",
    "for idx, row in diff_df.head(3).iterrows():\n",
    "    print(f\"  - {row['feature']}: {row['ratio']:.2f}x higher in attacks\")\n",
    "\n",
    "# 4. Security Recommendations\n",
    "print(\"\\n4. SECURITY RECOMMENDATIONS:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"• Implement real-time monitoring for top 3 critical features\")\n",
    "print(\"• Set up automated alerts for anomaly scores > 0.8\")\n",
    "print(\"• Focus manual investigation on low-confidence predictions\")\n",
    "print(\"• Regularly retrain models with new attack patterns\")\n",
    "\n",
    "# 5. Model Deployment Considerations\n",
    "print(\"\\n5. DEPLOYMENT CONSIDERATIONS:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"• False positive rate: {false_positives.mean():.2%} (may need tuning)\")\n",
    "print(f\"• False negative rate: {false_negatives.mean():.2%} (security risk)\")\n",
    "print(\"• Consider ensemble methods for critical applications\")\n",
    "print(\"• Implement human-in-the-loop for borderline cases\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Optimization and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "print(\"Performing hyperparameter optimization...\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search (using a subset for speed)\n",
    "rf_grid = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    rf_grid, param_grid, cv=3, scoring='roc_auc', \n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "# Use a subset of data for faster computation\n",
    "subset_size = min(1000, len(X_train_scaled))\n",
    "indices = np.random.choice(len(X_train_scaled), subset_size, replace=False)\n",
    "\n",
    "grid_search.fit(X_train_scaled[indices], y_train.iloc[indices])\n",
    "\n",
    "print(\"\\nBest parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"\\nBest cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate optimized model\n",
    "optimized_model = grid_search.best_estimator_\n",
    "optimized_model.fit(X_train_scaled, y_train)\n",
    "y_pred_opt = optimized_model.predict(X_test_scaled)\n",
    "y_pred_proba_opt = optimized_model.predict_proba(X_test_scaled)[:, 1]\n",
    "auc_opt = roc_auc_score(y_test, y_pred_proba_opt)\n",
    "\n",
    "print(f\"\\nOptimized model AUC: {auc_opt:.4f}\")\n",
    "print(f\"Original model AUC: {results['Random Forest']['auc']:.4f}\")\n",
    "print(f\"Improvement: {auc_opt - results['Random Forest']['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and results\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"/home/jovyan/output/ml_models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save best binary classifier\n",
    "best_model_obj = results[best_model_name]['model']\n",
    "joblib.dump(best_model_obj, f\"{output_dir}/best_binary_classifier_{timestamp}.pkl\")\n",
    "joblib.dump(scaler, f\"{output_dir}/feature_scaler_{timestamp}.pkl\")\n",
    "\n",
    "# Save multi-class classifier\n",
    "joblib.dump(rf_multiclass, f\"{output_dir}/multiclass_classifier_{timestamp}.pkl\")\n",
    "joblib.dump(le, f\"{output_dir}/label_encoder_{timestamp}.pkl\")\n",
    "\n",
    "# Save anomaly detection model\n",
    "joblib.dump(iso_forest, f\"{output_dir}/isolation_forest_{timestamp}.pkl\")\n",
    "\n",
    "# Save results summary\n",
    "results_summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'binary_classification': {\n",
    "        'best_model': best_model_name,\n",
    "        'best_auc': float(best_auc),\n",
    "        'all_results': {name: float(result['auc']) for name, result in results.items()}\n",
    "    },\n",
    "    'multiclass_classification': {\n",
    "        'accuracy': float((y_test_mc == y_pred_mc).mean()),\n",
    "        'classes': list(le.classes_)\n",
    "    },\n",
    "    'anomaly_detection': {\n",
    "        'performance': perf_df.to_dict('records')\n",
    "    },\n",
    "    'feature_importance': feature_importance.head(10).to_dict('records')\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/ml_results_summary_{timestamp}.json\", 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"✓ Models and results saved to {output_dir}\")\n",
    "print(f\"  • Binary classifier: best_binary_classifier_{timestamp}.pkl\")\n",
    "print(f\"  • Multi-class classifier: multiclass_classifier_{timestamp}.pkl\")\n",
    "print(f\"  • Anomaly detector: isolation_forest_{timestamp}.pkl\")\n",
    "print(f\"  • Results summary: ml_results_summary_{timestamp}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion and Next Steps\n",
    "\n",
    "### Summary\n",
    "This notebook demonstrated comprehensive machine learning applications for cybersecurity analytics:\n",
    "\n",
    "1. **Binary Classification**: Distinguished between normal and attack traffic\n",
    "2. **Multi-class Classification**: Identified specific attack types\n",
    "3. **Anomaly Detection**: Used multiple algorithms to find outliers\n",
    "4. **Feature Analysis**: Identified critical features for detection\n",
    "5. **Model Optimization**: Tuned hyperparameters for better performance\n",
    "\n",
    "### Key Findings\n",
    "- Random Forest performed best for binary classification\n",
    "- Feature importance revealed critical network characteristics\n",
    "- Different anomaly detection methods have different strengths\n",
    "- Model confidence correlates with specific features\n",
    "\n",
    "### Cybersecurity Applications\n",
    "- **Real-time Monitoring**: Deploy models for live traffic analysis\n",
    "- **Alert Systems**: Use prediction confidence for alert prioritization\n",
    "- **Threat Hunting**: Focus investigation on high-risk patterns\n",
    "- **Incident Response**: Quickly classify and prioritize security events\n",
    "\n",
    "### Next Steps for Students\n",
    "1. **Experiment with deep learning**: Try neural networks for comparison\n",
    "2. **Feature engineering**: Create domain-specific features\n",
    "3. **Ensemble methods**: Combine multiple models for better performance\n",
    "4. **Real-time implementation**: Develop streaming ML pipeline\n",
    "5. **Model interpretability**: Use SHAP or LIME for explainable AI\n",
    "\n",
    "### Production Considerations\n",
    "- **Model drift**: Monitor and retrain with new data\n",
    "- **Scalability**: Optimize for high-throughput environments\n",
    "- **False positive management**: Balance security vs. operational impact\n",
    "- **Integration**: Connect with SIEM and other security tools\n",
    "\n",
    "**Machine learning is a powerful tool for cybersecurity, but it requires continuous refinement and domain expertise to be effective in production environments.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}